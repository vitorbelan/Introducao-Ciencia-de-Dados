INTRODUCO A CIENCIA DE DADOS
    NOTAS DE AULA E REVISAO DE MODELOS

PARTE 1     ICD introducao a ciência de dados

    mELLO, r f Machine Learning: A pratical approach on the statistical learning theory
    sIMON HAYKIN, Neural Network : A comprehensive foundation
    Bishop, C.M Pattern Recognition and Machine Learning # primeiro contato
    Freman and Skapura, Neural Network: Algotithms, Applications and Programming Techiniques # livro mais introdutório antes do simon

    Faceli, kati; Lorena, Ana Carolina, Gama Joao: Inteligencia Artificial - Uma Abordagem de Aprendizado de Máquina

    Ciencia de dados
            Transformar grande quantidade ede dadso brutos em insights para o negócio e tomadas de decisao para atingir melhores resultados. Campo antigo porém se popularizou com o big data. Area interdisciplinar. 

    Mineraçao de Dados
            Procedo de explorar grandes quantidades de dados á procura de padrões consistentes, como regras de associaçao ou sequencias temporais para detectar relacionamentos sistemáticos entre variáveis e detectar novos subconjuntos de dados.

    Exemplo do que busca responder?
            Quais clientes te maior probabildiade em novas compras ?
            Quais devedores tem maior probabilidade em pagar ?
            Como prever a necessidade de um produto em estoque ?

    Espécie de padrão de projetos de ciëncia de dados:
        #Coleta de dados
            -Obter dados do problema tais como textos da web
            -Criar software para obtençao de dados

            -EX: criar um robot para obter noticias do mercado financeiro:
        #Preparaçao dos dados
            -Selecionar palavras de interesse
            -Computar frequencia de cada palavra
            -Ex: remover stopwords "preposições":
            -Organizacao ou estruturaçao dos dados
                -Remocao de dados desnecessarios
                -Preenchimento de dados faltantes
                    -Por aproximaçao em relaçao as outras instancias
                    -Eliminacao da instancia com dados faltantes
                    -até mesmo nao preencher
        #Modelagem dos dados
            - Criar um modelos para responder qual classe associdada a cada texto em funcao de um conjunto 
            - Ex: Rpresentar cada texto como um vetor num espaço, verificar documentos que sao mais similares:
            -Adoçao de uma estrategia de modelo baseada em
                Estatisticca processos estocasticos, Aprendizado de maquinas
    
    Primeiro exemplo:
        #Coleta de dados
            Dataset flor iris
        #Preparaçao dos dados
            Nao precisa preparar ou pré-procesar -->simplifica demais<--
        #Modelagem
            Adotar um modelos em hiperplanos separadores

        Num estudo : 
        as features, atributos, ou caracteristicas, -> neste exemplo sao suficiente para descrever (classificar - classe) as flores
            as features, atributos, variáveis de interesse ou caracteristicas: tamanho da petala, do caule, diametro
            classe: a partir das features descrevem o tipo da flor

        # Usando R
        uSANDO R um plot bidimensional de uma variável em funcao da outra
                cmd: plot(iris[,1:4], col = iris[,5]) # plotar as clunas 1 a 4 colorindo em funcao da coluna 5

        #preciso ter uma preucupaçao em entender o quao separável o meu espaço é
            -hiperplanos para separar as variaveis
            -histograma para ver o quao as variáveis sao separadas entre si
                ex: imagina enxergar as classes em funcao de um atributo só, como a largura e comprimeto de petala se comportam com a projeçao só em funçao do comprimento de pétala


    Ferramental Básico:
        #Histogramas
            -Analise dos histrogrmas de cada atributo como forma de observar a distribuicáo dos dados
            -Separacao entre classes pode se dar pela simples analise dos histogramas
        #Histogramas por atributos
                -Ex em r:
                cmd:
                p <- plot_ly(alpha - 0.6)
                + add_hisogram(x = iris[1:50,1])
                + add_hisogram(x = iris[1:100,1])
                + add_hisogram(x = iris[101:150,1])
                + layout(barmode + "overlay")
        
            quanto mais hiperplanos o modelo tiver muita chance de decorar o modelo ele terá overffiting

        #Correlaçao entre variáveis
            -Refere-se ao grau de relacionamento linear entre duas variaveis
            -ex: Correlaçao entre alturas de duas geraços de uma mesma familia
            -A correlaçao explorar relacionamento preditivos entre variáveis
                -Ex em r:
                cmd: co(1:10, 1:10)
            - Quando correlacao =1  igual a significa que x=y seguindo uma tendencia linear
            - Quando correlacao = - 1  igual a significa que x=-y uma anticorrelaçao 
            - Uma alta correlaçao entre variáveis signfica que posso escolher entre uma ou outra variável elas tem uma grande dependëncia entre elas
                -Ex em R:
                cmd: cor(iris[,1],iris[,3])

            -Correlaçao de PEARSON entre variáveis:
                rho(x,y) = covariancia(x,y)/(disviopadraox desviopadraoy)
                         = covariancia(variável subtraída da média)
                        
                        produto interno me retorna similaridade entre as variáveis

            - Correlaçao de SPEARMAN entre variáveis:
                Avalia quao bem o relacionamento entre duas variáveis pode ser descrito por uma funçao monotönica 

        #AutoValores ee AutoVetores
            -Maneira de compreender a relevanica de cada atributo no espaço de dados
            -Pérmite avaliar correlação entre atributos
                Eventualmente um atributo pode ser removido em duncao dele ja ser bem representado por outros atributos
            
            Autovetores
                aqueles que a transformaçao aplicada ao vetor muda somente a magnitude(comprimento) ou a direçao. Essa tranformaçao aplicada ao vetor nos diz 
                que ela atua sobre um  auto espaço linear

        #PCA - Principal Component Analysys 
            -Resultado natural da decomposicão de autovalores e autovetores
            -Muito utilizada para estudar variancias de atributos em espaçoes e suas releväncias
            -Olhar para um espaço com um monte de variaveis e descobrir quais dimensoes possui correlaçao entre essas dimensoes
                -Ex em R:
                cmd: require(splus2R)
                     data = as.data.frame(rmvnorm(100, rho = 0.9) * 5+10)
                     colnames(data) = c("Statistics", "Physics")
                     plot(data)
            # Para aplicarmo PCA precisamos fazer uma centralizaçao dos dados
                scaled = apply(data ,2 ,function(x) {x - mean(x)})
                plot(scaled)
            # pq centralizo os dados? 
                para evidenciar as relações vetoriais, pq depenendo dos dados eles podem acabar ficando tudo num mesmo quadrante
                dicas:
                    se as variáveis tiverem a mesma unidade de medida
                        # podemos simplesmente subtrair a média 'x-mean(x)'
                        # escreevemos a matrix de covariancia
                    se tiverem unidades distintas
                        # consideramos a centralizaçao pela média porém também dividimos pelo desvio padrao ('x-mean(x)')/sd(x)
                        # escrevemos a matriz de correlação  
                        # o intuito ao dividir é para diminuir ou reduzir a dominancia de uma variável em funçao da outra ja que estou dividindo ela pelo quanto ela varia, além de admensionalizar a variável (correçao de escala das variáveis)
                        # Porque usamos o produto interno
                        -quando gero o produto interno eu estou fazendo a projeçao de vetores, e estarei gerando  algo parecido como uma regressao linear, algo conhecido na geometria analitica como autoespaço linear, para saber o tipo de regressão linear que estao sendo obtidas nas variáveis
                        -no meu espaço centralizado a soma dos autovalores produz a soma das variancias, consequentemente >>>> meus autovalores produzem as variancias no meu espaço centralizado
                        -qual dimensao escolheria entao (levando em conta uma ideia na reducão de espaço) ? aquelas em que os autovalores sao mais importantes, os quais tem maior importancia relativa, maior autovalores
                        - usando o gráfico biplot se o elas possui angulo agudo entre si (produto interno positivo) elas sao correlatas, qualquer ponto entre esses dois vetores conseguem um equilibrio entre as duas variáveis
                        - um ponto é mais correlato a uma dada feature quando possuir um angulo agudo menor em relaçao à feature do vetor comparado

                aplicanddo sobre um outro dataset:
                    https://archive.ics.uci.edu/ >>>>>> cnj wine
                    cmd: r
                        dataset = read.table("wine.data", sep = ",")
                            1 - Y = dataset[,1] # sao as classes
                            2 - X = dataset[, 2:14] # sao meinhas features x
                            3 - plot(X , col = Y, pch = 20) # plot para mostrar as variaçoes entre cada feature
                            4 - scaled = apply(X, 2 , function(col) {(col -mean(col)/sd(col)) }) # plot para centralizar os produtos internos entre vetores posiçao, além da divisao em funçao do desvio padrão para corrigir diferenças de escalas e unidades 
                            5 - apply (scaled, 2, function(col) { range(col) }) # funçao utilizada para ver os ranges de cada coluna
                                    # talvez apagar? aqui é a construucao da matriz de mudança de base (de covariancia) responsável por distorcer os espaçoes e traze-los em função dos eixos ortogonais relevantes
                                # Descoberta dos eixos (ou auot-espaçoes lineares) sobre os quais a mtriaz de mmudança de base (,matriz de covariancia) opera permitindo que descubrams relaçoes lineares de dependencia linear entre atributos
                                # além disso a descoberta dos auto-valores associados aos auot-vetores com o intuito de descorbira as variancia relativas a cada auto-veotr
                            6 - scaled.cov = cov(scaled) # retorna a matriz de covariancia, matriz de 13x13 (cmd: dim(scaled.cov)), ou mais especifico do produto interno de cada feature em relaçao a outra, e todo possíveis autoespacos que podem ser obtidos nesse espaço R13
                            7 - eigens =eigen (scaled.cov) #variavel para produzir meus autovalores e autovetores
                            8 - eigens$values # autovalores >>>> tenho a importancia do componente principal 1 até o ultimo. sempre na ordem do mais importante para o menos importante
                            9 - eigens$vectors # autovetores >>>> tenho na primeira coluna o primeiro componente principal (mais principal), o vetor resultando do primeiro atributo é toda a primeira linha, e assim sucessivamente

                            ############ primeira analise

                            10 - sum(eigens$values) # me dão a soma das varianca do meu espaço scalado em funçao do meu atributo >>>> variancia total do meu espaço
                            11 - var[scaled[,1]]    # variancia em relaçao ao atributo 1
                            12 - var[scaled[,2]]    # variancia em relaçao ao atributo 2
                            13 - var[scaled[,3]]    # variancia em relaçao ao atributo 3 e assim para cada um dos atributos

                            14 - (eigens$values)/sum(eigens$values) # normalizado para ver a contribuiçao percental de cada novo atributo
                                                           
                    # Von Neumann
                    v.total = 0
                    for (i in 1: ncol(scaled)) { v.total = v.toal +var (14 - (eigens$values[,i])}
                    print(v.total)

                    von neuman normalizou os autovalores pela soma: ao fazer isso ele tem uma soma de probabilidade se somarmos todas teremos 1 como resultado
                        14.1 - (eigens$values)/sum(eigens$values) 

                    podemos medir a entropia da fonte de informaçao no caso a antena em R13
                        H(prob) = -sum(prob*log(prob)) # proposta por Shannon
                        prob = (eigens$values)/sum(eigens$values) 
                        -sum(prob*(log(prob)/log(2))) # quanto maior esse valor mais incerta é essa fonte dessa informação



                            15 - plot (eigens$values)/sum(eigens$values , pch =20, cex = 3) # plotando para ver a contribuiçao percentual
                            15 - plot (cumsum(eigens$values)/sum(eigens$values) , pch =20, cex = 3) # plotando para ver a soma acumulada da contribuiçao percentual
                            16 - abline (v=8, h=0.9)    # criando uma linha para analise


                            17 - thinking by yourself   # se eu pegar os 8 primeiros componentes principais eu estou reduzindo a dimensionalidade do meu espaço, mas estarei mantendo cerca de  90% da informaçao revelante do meu espaço que é representada VARIÄNCIA DO meu espaço.  A VARIANCIA REPRESENTA A QUANTIDADE DE INFORMAÇAO RELEVANTE QUE EU TENHO. SE eu usar 4 componentes vou manter cerca de 70% da informação relevante do meu espaço. 95% [E UM PONTO DE CORTE PARA A INFORMAÇAO RELEVANTE QUE GRANDE PARTE DA LITERATURA USA]
                            18.1 -VOU FAZER A ROTAÇAO DOS MEUS DADOS                           
                            18.2 - as.numeric(scaled[1, ]) # vetor do meu primeiro atributo
                            18.3 - eigen$vectores%*%as.numeric(scaled[1, ]) # matriz de mudança de base do primeiro vetor
                            #Transformaçao linear sobre os vetores linha da matriz scaled tento aplicado sobre cada vetor os auto-valores
                            19 - scores = scaled %*% eigens$vectores #matriz de mudança de base de todos meu atributos precisei trocar por causa das dimensoes 1x13 * 13x13 isso é o gráfico de scores o que foi feito? rotacionado o espaço em funçao dos componentes principais
                            20 - scores = as.data.frame(scaled %*% eigen$vectores) # transformando em um dataframe
                            21 - colnames(scores) = paste("PC",1:13,sep = "") # colocando o nome de cada coluna como PC1, PC2, PC3 >>>Principal Component
                            22 - colnames(scores)
                            23 - scores[,1:9] # pegue da primeira coluna até a 9 pq depois da 9 coluna tem uma uantidade de informaçao muito pequena
                            24 - plot(scores[,1:4] , col =Y)  # gráfico de assumindo somente as variaçoes entre as 4 principais variáveis
                                17.1 - automatizando a escolha  dos principais componentes algo parecdo com os comandos 20,21,22,23,24:
                                    r = cumsum(eigens$values/sum(eigens$values) )
                                    id = which( r>= 0.9)[1]
                                    x.reduzida = scores[,1:id]
                                    plot(x.reduzia, col =Y)

        Aprendizado baseado em instancia:
            Vou definidir uma nova instäncia a partir dos pontos vizinhos mais próximos
            # Algoritmo de k-Nearest Neighbors
            -tecnica mas comum do IBL (Instance-Based Learnin)
            -calcula a distancia euclidinada da nova instancia(novo ponto a ser conhecido) em funcao de cada instancia na base de conhecimento
            -Seleciona as k instancia de treinamento mais próxima
            -define o atributo de saida na forma
                -Discreta:
                    ex: o maior numero de votos decide-se sea a cor é vermelha ou verde
                -Continua:
                    ex: define se o atributo de saida pela ponderacao das saidas das K instancias mais proximas
            
            vai contar quantos "votos" eu tenho para cada i-ésimo vizinho do cenário e faz uma média
                ex: em R
                            X       -> CNJ DE ENTRADAS
                            Y       -> CLASSE ASSOCIADA A CADA EXEMPLO
                            K      -> NUMERO DE VIZINHOS MAIS PRÓXIMO QUE DESEJO ENCONTRAR
                            query   -> PONTO DE CONSULTA; meu novo ponto que desejo classificar
                            #
                            # Supoe que o espaço de atrtibutos de entrada X é informtativo
                            # para prosseguir com o processo de classificaçao   
                            #

                            ###########################
                            # k-Nearest Neighbors
                            ###########################

                            # - Algoritimo de aprendizado supervisionado
                            # f(x) = y
                            # f:X -> Y mapea elemntos que estao no cjn x em y, f é um classificados ou funçao classficadora

                            # O conjunto X é chamado de espaço de entradas
                            # O conjunto Y é chamado de espaço de saídas
                            #             Y É discreto para problemas de classificaçao numero finito de possibilidades



                            knn <- function(K, X, Y) {
                                # distancia euclidiana que a consulta( query) tem em relacao
                                # aos exemmplos ou linhas da matriz X
                                E = apply(X, 1, function(row) {sqrt(summ((row - query)^2)) }) # distancia euclianda que cada mlinha do meu datset tem em relacao a minha consulta 
                                ids = sort.list(E, dec = F)[1:K] # vai trazer as posicões em x que estao mais orõxima do meu ponto de consulta
                                
                                classes = unique(Y) # vai tirar as repeticoes e manter somente elementos unicos
                                count   = rep(0, lenght(classes)) # VAI TRAZER A quantidade de votos que cada classe teve 
                                for (class in classes){
                                        countt[i] = sum( class == Y[ids]) # A soma de votos por classe
                                        i = i + 1
                                }

                                ret = list()
                                ret$classes = classes
                                ret$count = count
                                ret$max.prob.class = classes[which.max(count)] # retorna a posicao no vetor da classe que contem a maior qtd de votos

                                return (ret)

                            }

                            tes.wine <- function(PCA = FALSE, perc =0.9) {

                                # PCA == FALSE ----> sem transformaçao por auto-valores/auto-vetores
                                # PCA == TRUE  ----> com transformacao e selecionando umm subconjunto dos PCs   

                                #carrega o conjunto de dado wine
                                dataset = read.table("wine.data", sep = ",")

                                #monta os espacoes de entrada e saida
                                X = dataset[,2:14]  # espaço de entrada
                                Y = dataset[,1]     # espaço de saida

                                if (CPA) {
                                    # Transformaçao especial

                                    #Centralizacao e re-escla dos atributos/variaveis/features
                                    scaled = apply (X, 2, function(col) { (col - mean(col))/sd(col)})

                                    #Avalie as dependencias lineares entre os atributos
                                    C = cov(scaled) #13x13

                                    #Decomposiçao de auto-valores e auto-vetores
                                    E = eigen(C)

                                    plot(cumsum(E$values/sum(E%values)), main = 'Importancia agregada por PC', xlab = "# PCs", ylab = "Importancia %")
                                    locator(1)

                                    numero.PCs = which(cumsum(E$values/sum(E%values)) >= perc)[1]

                                    Transform = scaled %*% E%vectores
                                    X = Transform [,1:numero.PCs]
                                    
                                }


                                # res = knn(query=X[1,], k=5, X=X, Y=Y) parte do codigo restrita a priemira parte da  aula 2
                                # print(res)

                                R = matrix(0, nrow = 10, ncol=5)

                                for (j in 1:5) {

                                    #######
                                    #Hold -out
                                    #######

                                    # - uma parcela dos dados é escolhida aleatoriamente (uniforme) para compor o conjunto de treinamento
                                    # e o restante disjunto dos exemplos irá compor o conjunto de testes

                                        #conjunto de treinamento
                                        ids = samples(1:nrow(X), size = ceiling(0.5*nrow(X)))
                                        X.train = X[ids,]           # base de conhecimento (knowledge base)
                                        Y.train = Y[ids]            # compoem as classes ou rotulos dos exemplos da base de conhecimento

                                        # Conjunto de teste 
                                        X.test = X[-ids,]           # pega ccomplemento desse conjunto
                                        Y.test = Y[-ids]

                                        R= NULL

                                        for (k in 1:10){
                                                    cat("Running for k = ", k , "\n")
                                                    accuracy = 0    # taxa de acerto     
                                                    for (i in 1:nrow(X.test)) {

                                                                res = knn(query=X[i,], k=k, X=X.train, Y=Y.train) # testo meu modelo de algo que nao foi visto
                                                                if (res$max.prob.class = Y.tes[i]){
                                                                    accuracy = accuracy + 1
                                                                }
                                                    }

                                                    accuracy = accuracy/nrow(X.test)
                                                    R[k,j] = accuracy
                                                    # R = rbind(R, C(k, accuracy))
                                                    # cat("Accuracy = ", accuraci, "\n")
                                        }
                                }
                                # plot(R, xlab = "k values", ylab = "Accuracy", cex =2, pch =20, cex.lab =2)

                                return (R)

                                }

            # DWNN - Distance Weighted Neares Neighbors
            # ex 4
                Regressao do espaço
                Um refinamentoe ou variaçao do KNN
                Considera uma ponderaçao de cada vizinho em funcao de sua distancia à nova instancia
                aO INVES DE CONSIDERAR pontos próximo ele cria gaussianas em volta, criando um raio
                A reposta ou classe ou rotulo vai ser um média ponderada das saídas produzidas por seus vizinhos mais próximos que também sao saídas continuas

                    ###########################
                    # Distance Weighted Neares Neighbors
                    ###########################

                    # - Algoritimo de aprendizado supervisionado
                    # f(x) = y , f é um regressor ou funçao regressora
                    # f:X -> Y mapea elemntos que estao no cjn x em y, 

                    # O conjunto X é chamado de espaço de entradas
                    # O conjunto Y é chamado de espaço de saídas
                    #             Y É continuo para problemas de regressao
                    #             numero infinito de possibilidades
                    #
                    # sigma -> inf
                        # - vou produzir como resposta da regressao a média de toas as respotas dos ecemplo da minha knoledge base

                    # sigma -> 0
                        # - vou prozuir a NaN pois nao haverá qualquer vizinho mais próximo de um ponto que nao seja um pertencente a propria knoledge base

                    dwnn <- function(query, X, Y, sigma) {
                        E = apply(X, 1, function(row) {sqrt(sum((row - query)^2))} ) # calculo a distancia euclidiana novamente
                        weight = exp(-E^2 / 2*sigma^2) # sempre entre 0 e 1 se tiver exatamente sobre o ponto o 1 vai signifiacr que estu numa distancia 0
                                                        # e se superdistante vai dar distancia 0
                        return(weight %*% Y/ sum(weight))

                    }

                    test.linear <- function(sigma) {
                        dataset = cbind(-5:5, -5:5)
                        print(dataset)
                        plot(dataser, pch=20)

                        X = (dataset[,1], ncol = 1)
                        Y = (dataset[,2], ncol = 1)
                        R = NULL

                        for (x in seq(-5,5, len=100)){
                            y = dwnn(query = x, X=X, Y=Y, sigma=sigma)
                            R =rbind(R, c(x, y))

                        }

                        pints(R, col=2, pch=20)


                    }

                    # teste linear ampliando o espaço de entrda X nas consultas
                    # a dsitribiçao de probabilidade conjunta de P(x,y) no treinamento é distinta da distribuiçao do teste

                    
                    test.linear2 <- function(sigma) {
                        dataset = cbind(-5:5, -5:5)
                        print(dataset)
                        plot(dataser, pch=20,cex.lab = 2,cex.axis = 2, xlim = c(-7,7))

                        X = (dataset[,1], ncol = 1)
                        Y = (dataset[,2], ncol = 1)
                        R = NULL

                        for (x in seq(-7,7, len=100)){
                            y = dwnn(query = x, X=X, Y=Y, sigma=sigma)
                            R =rbind(R, c(x, y))

                        }

                        pints(R, col=2, pch=20)


                    }
                    # Regressao de uma série temporal

                    test.sin <- function(sigma) {
                        time = seq(0,9, length = 1000)
                        series = sin(2*pi*time) # unidimensional
                        par(mfrow=c(2,1))
                        plot(series, xlim = c(0,1200), pch = 20, cex = 2, cex.lab = 2)

                        # x             y
                        # series[1], series[2]
                        # series[2], series[3]
                        # series[3], series[4]

                        # espaço de entradas X
                        X = matrix(series[1:999], ncol = 1)

                        # espaço de saidas Y
                        Y = as.numeric(series[2:1000])

                        # Conjunto de treinamento
                        id = 1:800
                        X.train = matrix(X([id,], ncol=1))
                        Y.train = Y[id,]

                        # cONJUNTO DE TESTE
                        X.test = matrix(X([id,], ncol=1))
                        Y.test = Y[-id,]

                        #Testing
                        R = NULL
                        for (i in 1:nrow(X.test)) {
                            x = X.test[i,]
                            y = dwnn(query = x, X = X.train, Y= Y.train, sigma = sigma)
                            R = rbind(R, c(x,y))

                        }

                        plot(series[-id], pch = 20, cex = 2, cex.lab = 2)
                        points(R[,2], col=2 , pch = 20)
                    }


                    test.sin2 <- function(sigma) {
                        time = seq(0,9, length = 1000)
                        series = sin(2*pi*time) # unidimensional
                        par(mfrow=c(1,1))
                        plot(series, xlim = c(0,1200), pch = 20, cex = 2, cex.lab = 2)
                        locator(1)

                        # x             y
                        # series[1], series[2]
                        # series[2], series[3]
                        # series[3], series[4]

                        # espaço de entradas X
                        X = matrix(series[1:999], ncol = 1)

                        # espaço de saidas Y
                        Y = as.numeric(series[2:1000])

                        # Conjunto de treinamento
                        id = 1:800
                        X.train = X
                        Y.train = Y

                        # pREDIÇAO RECORRENTE ---> redes neurais profundas/ Deep Learning
                        #                          uma saida vira entrada para prever uma proxima saida
                        R = matrix(NA, nrow = row(X), ncol = 2)
                        x = series[1000] #y[999]
                         for (I in 1:lag.max ){
                            dwnn(query = x, X = X.train, Y = Y.train, sigma = sigma)
                            R = rbind(R, c(x,y))
                            x=y

                         }

                         points(R[,2], pch= 20, cexx = 2)

}




             