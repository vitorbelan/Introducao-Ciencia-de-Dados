INTRODUCO A CIENCIA DE DADOS
    NOTAS DE AULA E REVISAO DE MODELOS

PARTE 1     ICD introducao a ciência de dados

    mELLO, r f Machine Learning: A pratical approach on the statistical learning theory
    sIMON HAYKIN, Neural Network : A comprehensive foundation
    Bishop, C.M Pattern Recognition and Machine Learning # primeiro contato
    Freman and Skapura, Neural Network: Algotithms, Applications and Programming Techiniques # livro mais introdutório antes do simon

    Faceli, kati; Lorena, Ana Carolina, Gama Joao: Inteligencia Artificial - Uma Abordagem de Aprendizado de Máquina

    Ciencia de dados
            Transformar grande quantidade ede dadso brutos em insights para o negócio e tomadas de decisao para atingir melhores resultados. Campo antigo porém se popularizou com o big data. Area interdisciplinar. 

    Mineraçao de Dados
            Procedo de explorar grandes quantidades de dados á procura de padrões consistentes, como regras de associaçao ou sequencias temporais para detectar relacionamentos sistemáticos entre variáveis e detectar novos subconjuntos de dados.

    Exemplo do que busca responder?
            Quais clientes te maior probabildiade em novas compras ?
            Quais devedores tem maior probabilidade em pagar ?
            Como prever a necessidade de um produto em estoque ?

    Espécie de padrão de projetos de ciëncia de dados:
        #Coleta de dados
            -Obter dados do problema tais como textos da web
            -Criar software para obtençao de dados

            -EX: criar um robot para obter noticias do mercado financeiro:
        #Preparaçao dos dados
            -Selecionar palavras de interesse
            -Computar frequencia de cada palavra
            -Ex: remover stopwords "preposições":
            -Organizacao ou estruturaçao dos dados
                -Remocao de dados desnecessarios
                -Preenchimento de dados faltantes
                    -Por aproximaçao em relaçao as outras instancias
                    -Eliminacao da instancia com dados faltantes
                    -até mesmo nao preencher
        #Modelagem dos dados
            - Criar um modelos para responder qual classe associdada a cada texto em funcao de um conjunto 
            - Ex: Rpresentar cada texto como um vetor num espaço, verificar documentos que sao mais similares:
            -Adoçao de uma estrategia de modelo baseada em
                Estatisticca processos estocasticos, Aprendizado de maquinas
    
    Primeiro exemplo:
        #Coleta de dados
            Dataset flor iris
        #Preparaçao dos dados
            Nao precisa preparar ou pré-procesar -->simplifica demais<--
        #Modelagem
            Adotar um modelos em hiperplanos separadores

        Num estudo : 
        as features, atributos, ou caracteristicas, -> neste exemplo sao suficiente para descrever (classificar - classe) as flores
            as features, atributos, variáveis de interesse ou caracteristicas: tamanho da petala, do caule, diametro
            classe: a partir das features descrevem o tipo da flor

        # Usando R
        uSANDO R um plot bidimensional de uma variável em funcao da outra
                cmd: plot(iris[,1:4], col = iris[,5]) # plotar as clunas 1 a 4 colorindo em funcao da coluna 5

        #preciso ter uma preucupaçao em entender o quao separável o meu espaço é
            -hiperplanos para separar as variaveis
            -histograma para ver o quao as variáveis sao separadas entre si
                ex: imagina enxergar as classes em funcao de um atributo só, como a largura e comprimeto de petala se comportam com a projeçao só em funçao do comprimento de pétala


    Ferramental Básico:
        #Histogramas
            -Analise dos histrogrmas de cada atributo como forma de observar a distribuicáo dos dados
            -Separacao entre classes pode se dar pela simples analise dos histogramas
        #Histogramas por atributos
                -Ex em r:
                cmd:
                p <- plot_ly(alpha - 0.6)
                + add_hisogram(x = iris[1:50,1])
                + add_hisogram(x = iris[1:100,1])
                + add_hisogram(x = iris[101:150,1])
                + layout(barmode + "overlay")
        
            quanto mais hiperplanos o modelo tiver muita chance de decorar o modelo ele terá overffiting

        #Correlaçao entre variáveis
            -Refere-se ao grau de relacionamento linear entre duas variaveis
            -ex: Correlaçao entre alturas de duas geraços de uma mesma familia
            -A correlaçao explorar relacionamento preditivos entre variáveis
                -Ex em r:
                cmd: co(1:10, 1:10)
            - Quando correlacao =1  igual a significa que x=y seguindo uma tendencia linear
            - Quando correlacao = - 1  igual a significa que x=-y uma anticorrelaçao 
            - Uma alta correlaçao entre variáveis signfica que posso escolher entre uma ou outra variável elas tem uma grande dependëncia entre elas
                -Ex em R:
                cmd: cor(iris[,1],iris[,3])

            -Correlaçao de PEARSON entre variáveis:
                rho(x,y) = covariancia(x,y)/(disviopadraox desviopadraoy)
                         = covariancia(variável subtraída da média)
                        
                        produto interno me retorna similaridade entre as variáveis

            - Correlaçao de SPEARMAN entre variáveis:
                Avalia quao bem o relacionamento entre duas variáveis pode ser descrito por uma funçao monotönica 

        #AutoValores ee AutoVetores
            -Maneira de compreender a relevanica de cada atributo no espaço de dados
            -Pérmite avaliar correlação entre atributos
                Eventualmente um atributo pode ser removido em duncao dele ja ser bem representado por outros atributos
            
            Autovetores
                aqueles que a transformaçao aplicada ao vetor muda somente a magnitude(comprimento) ou a direçao. Essa tranformaçao aplicada ao vetor nos diz 
                que ela atua sobre um  auto espaço linear

        #PCA - Principal Component Analysys 
            -Resultado natural da decomposicão de autovalores e autovetores
            -Muito utilizada para estudar variancias de atributos em espaçoes e suas releväncias
            -Olhar para um espaço com um monte de variaveis e descobrir quais dimensoes possui correlaçao entre essas dimensoes
                -Ex em R:
                cmd: require(splus2R)
                     data = as.data.frame(rmvnorm(100, rho = 0.9) * 5+10)
                     colnames(data) = c("Statistics", "Physics")
                     plot(data)
            # Para aplicarmo PCA precisamos fazer uma centralizaçao dos dados
                scaled = apply(data ,2 ,function(x) {x - mean(x)})
                plot(scaled)
            # pq centralizo os dados? 
                para evidenciar as relações vetoriais, pq depenendo dos dados eles podem acabar ficando tudo num mesmo quadrante
                dicas:
                    se as variáveis tiverem a mesma unidade de medida
                        # podemos simplesmente subtrair a média 'x-mean(x)'
                        # escreevemos a matrix de covariancia
                    se tiverem unidades distintas
                        # consideramos a centralizaçao pela média porém também dividimos pelo desvio padrao ('x-mean(x)')/sd(x)
                        # escrevemos a matriz de correlação  
                        # o intuito ao dividir é para diminuir ou reduzir a dominancia de uma variável em funçao da outra ja que estou dividindo ela pelo quanto ela varia, além de admensionalizar a variável (correçao de escala das variáveis)
                        # Porque usamos o produto interno
                        -quando gero o produto interno eu estou fazendo a projeçao de vetores, e estarei gerando  algo parecido como uma regressao linear, algo conhecido na geometria analitica como autoespaço linear, para saber o tipo de regressão linear que estao sendo obtidas nas variáveis
                        -no meu espaço centralizado a soma dos autovalores produz a soma das variancias, consequentemente >>>> meus autovalores produzem as variancias no meu espaço centralizado
                        -qual dimensao escolheria entao (levando em conta uma ideia na reducão de espaço) ? aquelas em que os autovalores sao mais importantes, os quais tem maior importancia relativa, maior autovalores
                        - usando o gráfico biplot se o elas possui angulo agudo entre si (produto interno positivo) elas sao correlatas, qualquer ponto entre esses dois vetores conseguem um equilibrio entre as duas variáveis
                        - um ponto é mais correlato a uma dada feature quando possuir um angulo agudo menor em relaçao à feature do vetor comparado

                aplicanddo sobre um outro dataset:
                    https://archive.ics.uci.edu/ >>>>>> cnj wine
                    cmd: r
                        dataset = read.table("wine.data", sep = ",")
                            1 - Y = dataset[,1] # sao as classes
                            2 - X = dataset[, 2:14] # sao meinhas features x
                            3 - plot(X , col = Y, pch = 20) # plot para mostrar as variaçoes entre cada feature
                            4 - scaled = apply(X, 2 , function(col) {(col -mean(col)/sd(col)) }) # plot para centralizar os produtos internos entre vetores posiçao, além da divisao em funçao do desvio padrão para corrigir diferenças de escalas e unidades 
                            5 - apply (scaled, 2, function(col) { range(col) }) # funçao utilizada para ver os ranges de cada coluna
                                    # talvez apagar? aqui é a construucao da matriz de mudança de base (de covariancia) responsável por distorcer os espaçoes e traze-los em função dos eixos ortogonais relevantes
                                # Descoberta dos eixos (ou auot-espaçoes lineares) sobre os quais a mtriaz de mmudança de base (,matriz de covariancia) opera permitindo que descubrams relaçoes lineares de dependencia linear entre atributos
                                # além disso a descoberta dos auto-valores associados aos auot-vetores com o intuito de descorbira as variancia relativas a cada auto-veotr
                            6 - scaled.cov = cov(scaled) # retorna a matriz de covariancia, matriz de 13x13 (cmd: dim(scaled.cov)), ou mais especifico do produto interno de cada feature em relaçao a outra, e todo possíveis autoespacos que podem ser obtidos nesse espaço R13
                            7 - eigens =eigen (scaled.cov) #variavel para produzir meus autovalores e autovetores
                            8 - eigens$values # autovalores >>>> tenho a importancia do componente principal 1 até o ultimo. sempre na ordem do mais importante para o menos importante
                            9 - eigens$vectors # autovetores >>>> tenho na primeira coluna o primeiro componente principal (mais principal), o vetor resultando do primeiro atributo é toda a primeira linha, e assim sucessivamente

                            ############ primeira analise

                            10 - sum(eigens$values) # me dão a soma das varianca do meu espaço scalado em funçao do meu atributo >>>> variancia total do meu espaço
                            11 - var[scaled[,1]]    # variancia em relaçao ao atributo 1
                            12 - var[scaled[,2]]    # variancia em relaçao ao atributo 2
                            13 - var[scaled[,3]]    # variancia em relaçao ao atributo 3 e assim para cada um dos atributos

                            14 - (eigens$values)/sum(eigens$values) # normalizado para ver a contribuiçao percental de cada novo atributo
                                                           
                            # Von Neumann
                            v.total = 0
                            for (i in 1: ncol(scaled)) { v.total = v.toal +var (14 - (eigens$values[,i])}
                            print(v.total)


                            15 - plot (eigens$values)/sum(eigens$values , pch =20, cex = 3) # plotando para ver a contribuiçao percentual
                            15 - plot (cumsum(eigens$values)/sum(eigens$values) , pch =20, cex = 3) # plotando para ver a soma acumulada da contribuiçao percentual
                            16 - abline (v=8, h=0.9)    # criando uma linha para analise


                            17 - thinking by yourself   # se eu pegar os 8 primeiros componentes principais eu estou reduzindo a dimensionalidade do meu espaço, mas estarei mantendo cerca de  90% da informaçao revelante do meu espaço que é representada VARIÄNCIA DO meu espaço.  A VARIANCIA REPRESENTA A QUANTIDADE DE INFORMAÇAO RELEVANTE QUE EU TENHO. SE eu usar 4 componentes vou manter cerca de 70% da informação relevante do meu espaço. 95% [E UM PONTO DE CORTE PARA A INFORMAÇAO RELEVANTE QUE GRANDE PARTE DA LITERATURA USA]
                            18.1 -VOU FAZER A ROTAÇAO DOS MEUS DADOS                           
                            18.2 - as.numeric(scaled[1, ]) # vetor do meu primeiro atributo
                            18.3 - eigen$vectores%*%as.numeric(scaled[1, ]) # matriz de mudança de base do primeiro vetor
                            #Transformaçao linear sobre os vetores linha da matriz scaled tento aplicado sobre cada vetor os auto-valores
                            19 - scores = scaled %*% eigens$vectores #matriz de mudança de base de todos meu atributos precisei trocar por causa das dimensoes 1x13 * 13x13 isso é o gráfico de scores o que foi feito? rotacionado o espaço em funçao dos componentes principais
                            20 - scores = as.data.frame(scaled %*% eigen$vectores) # transformando em um dataframe
                            21 - colnames(scores) = paste("PC",1:13,sep = "") # colocando o nome de cada coluna como PC1, PC2, PC3 >>>Principal Component
                            22 - colnames(scores)
                            23 - scores[,1:9] # pegue da primeira coluna até a 9 pq depois da 9 coluna tem uma uantidade de informaçao muito pequena
                            24 - plot(scores[,1:4] , col =Y)  # gráfico de assumindo somente as variaçoes entre as 4 principais variáveis
                                17.1 - automatizando a escolha  dos principais componentes algo parecdo com os comandos 20,21,22,23,24:
                                    r = cumsum(eigens$values/sum(eigens$values) )
                                    id = which( r>= 0.9)[1]
                                    x.reduzida = scores[,1:id]
                                    plot(x.reduzia, col =Y)

                            







             